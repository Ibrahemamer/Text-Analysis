{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (EM) Algorithm Applied to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial summarizes how the EM algorithm can be applied to texts. It is based on Stephen Hansen's lecture notes <a>https://sekhansen.github.io/pdf_files/lecture2.pdf</a> and text-mining module <a>https://github.com/sekhansen/text-mining-tutorial<a/>.\n",
    "\n",
    "[**Just Give Me the Code**](#gimmethecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, suppose we have 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[\"rugby\",\"football\",\"competition\",\"ball\",\"games\"],\n",
    "        [\"macro\",\"economics\",\"io\",\"competition\",\"games\",\"econometrics\"],\n",
    "        [\"business\",\"economics\",\"stocks\",\"bonds\",\"NYSE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, document 1 is a <i>sports</i> document, document 2 is related to <i>economics</i>, and document 3 is related to <i>business</i>. \n",
    "\n",
    "**Goal**\n",
    "1. Obtain Topic Distribution <br>\n",
    "The goal is to find the distribution of K topics. That is, in our corpus what is the probability a random document is related to topic 1 or topic 2 or ... topic K? This is represented by $\\rho$=$(\\rho_1,\\rho_2,...\\rho_K)$ where $1\\geq\\rho_k\\geq0$. This is just a vector of size K. \n",
    "<br>\n",
    "<br>\n",
    "2. Obtain Word Distrubition <br>\n",
    "Given a topic $k$, what is the probability that the document contains any word in our vocabulary of size V. This is represented by $\\mathbf\\beta_k$=($\\beta_{1,k},\\beta_{2,k},...,\\beta_{V,k})$ where $1\\geq\\beta_{v,k}\\geq0$. We will have K such vectors, so $\\mathbf\\beta$ will be a matrix of size K by V.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will assume that the probability of observing a document $w_d$ is: $Pr(w_d | \\rho, \\mathbf\\beta)=\\Sigma_{k=1}^K(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$, where $x_{v,d}$ is the occurences of word v in document d.\n",
    "\n",
    "\n",
    "So the probability of observing document 1 is just: <br>$Pr$([\"rugby\",\"football\",\"competition\",\"ball\",\"games\"] | $\\rho, \\mathbf\\beta)=\\rho_{1}(\\beta_{ball,1}^{1}*\\beta_{bonds,1}^{0}*...)+\\rho_{2}(\\beta_{ball,2}^{1}*\\beta_{bonds,2}^{0}*...)$\n",
    "\n",
    "We can represent the data in a frequency matrix where each row is a document, and each column is a word the vocabulary. The values correspond to the frequencies a word shows up in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ball  bonds  business  competition  economics  football  games  macro  rugby  stocks\n",
      "0     1      0         0            1          0         1      1      0      1       0\n",
      "1     0      0         0            1          1         0      1      1      0       0\n",
      "2     0      1         1            0          1         0      0      0      0       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "feature_counts = pd.DataFrame(data=\n",
    "                [[ 1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.],                            \n",
    "                 [ 0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.],\n",
    "                 [ 0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]], \n",
    "                columns=[u'ball', u'bonds', u'business', u'competition', u'economics',                   \n",
    "                         u'football', u'games', u'macro', u'rugby', u'stocks'])\n",
    "\n",
    "print(feature_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "The EM algorithm starts by guessing the probability that a document comes from topic $k$. Let's define the number of topics, K, and our initial guess of the distribution of each topic.\n",
    "\n",
    "We import numpy as this module gives us a lot of useful functions like linear algebra operations and sampling from different types of distributions. \n",
    "\n",
    "<b>k</b> is the number of (unknown or latent) topics.\n",
    "\n",
    "<b>rhos</b> ($\\rho$) is the initial \"guess\" of the distribution of topics. The uniform distribution is a good starting point. We'll guess the probability of a document belonging to topic 1 is 50% and topic 2 is 50%. \n",
    "\n",
    "<b>betas</b> ($\\mathbf\\beta$) we will also guess , the probability matrix where word $v$ appears in topic $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 2\n",
    "\n",
    "D = feature_counts.shape[0]\n",
    "\n",
    "V = feature_counts.shape[1]\n",
    "\n",
    "rhos = np.full(K,1/float(K))\n",
    "\n",
    "betas = np.random.dirichlet(V*[1], K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Summary of Algorithm##\n",
    "$Pr(w_d | \\rho, \\mathbf\\beta)$ is the likelihood of observing an entire document. Though consider that we are summing over topics. So we can separate each $(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$ and interpret this loosely as how much that document <i>likes</i> topic k. If $(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$ is really high, then according to the distributional parameters $\\beta$ and $\\rho$, document $w_d$ is a good candidate to be in topic $k$. So we'll proceed first by computing a matrix showing how much each document <i>likes</i> each of the $k$ topics.\n",
    "\n",
    "**Step 1: Expectation Step**\n",
    "<br> $\\text{Document Topic Matrix} \n",
    "\\begin{equation*}\n",
    " =  \\begin{vmatrix}\n",
    "\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc1}} & \\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc1}} \\\\\n",
    "\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc2}} & \\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc2}} \\\\\n",
    "\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc3}} & \\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc3}} \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "We will normalize this Document Topic Matrix so that the rows sum to 1. Then, each row element $i,j$ can be viewed as a probability that document $i$ belongs to topic $j$. We will call this the weighting matrix, $z$.\n",
    "\n",
    "Importantly, at the E-Step we should compute the total likelihood to see how likely the entire corpus of documents would occur under the current estimates of $\\beta$ and $\\rho$. If our likelihood only increases by a small amount, we can be happy assuming the current estimates of $\\beta$ and $\\rho$. \n",
    "\n",
    "**Step 2: Maximization Step**\n",
    "<br>\n",
    "\n",
    "$ z \\begin{equation*}\n",
    " =  \\begin{vmatrix}\n",
    "\\frac{\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc1}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc1}}}\n",
    "& \\frac{\\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc1}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc1}}}\n",
    " \\\\\n",
    "\\frac{\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc2}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc2}}}\n",
    "& \\frac{\\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc2}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc2}}} \\\\\n",
    "\\frac{\\rho_{topic1} \\Pi_{v=1}^V\\beta_{v,topic1}^{x_{v,doc3}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc3}}}\n",
    "& \\frac{\\rho_{topic2} \\Pi_{v=1}^V\\beta_{v,topic2}^{x_{v,doc3}}}{\\Sigma_k \\rho_{k} \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,doc3}}}\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "We can average the $z$ matrix of probabilities <i>across</i> documents to get anew estimate of $\\rho$. \n",
    "\n",
    "Similarly, the normalized Document Topic Matrix can be used to update $\\beta$. We will simply multiply the z matrix by the document term matrix above (with the counts of each word in each document. The z matrix is DxK, and the frequency count matrix is DxV, so we need to transpose the z matrix. Then will will also normalize it so that the rows of the final KxV matrix sum to 1. \n",
    "\n",
    "### E-Step###\n",
    "\n",
    "First let's see how much each document likes each topic. We do this by computing each documents likelihood, given our initial guess of $\\rho$ and $\\beta$. We will log these likelihoods then exponentiate them later.\n",
    "\n",
    "Log-Likelihood of document $w_d$:\n",
    "$log(Pr(w_d | \\rho, \\mathbf\\beta))=\\Sigma_{k=1}^K(log(\\rho_k)+ \\Sigma_{v=1}^Vlog(\\beta_{v,k})*{x_{v,d}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_temp = np.zeros((D,K))\n",
    "for doc in range(D):\n",
    "    for topic in range(K):\n",
    "        dt_temp[doc,topic] = np.log(rhos[topic]) + (feature_counts.values[doc]*np.log(betas[topic])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the log world so we need to exponentiate the likelihoods to retrieve $\\Sigma_{k=1}(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.29264425e-07   1.96682140e-08]\n",
      " [  2.39421947e-05   1.31566407e-05]\n",
      " [  1.28002718e-05   1.63341619e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(dt_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extremely small numbers. We will eventually need to calculate the complete likelihood over all documents, that is $l(X|\\rho\\beta) = \\Sigma_{d=1}^{D} log(\\Sigma_{k=1}^K\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})=\\Sigma_{d=1}^{D} log(\\Sigma_{k=1}^Ke^{(log(\\rho_k)+ \\Sigma_{v=1}^Vlog(\\beta_{v,k})*{x_{v,d}})})$. Taking the logarithms of such small numbers, as in the print above, will lead to <b>numerical underflow</b>. Fortunately, we can use the <b>log-sum trick</b>, which is factoring out the largest exponent:\n",
    "\n",
    "$log\\Sigma_{c}e^{b_c} = log[(\\Sigma_{c}e^{b_c-B})e^B]=[log(\\Sigma_ce^{b_c-B})]+B$, where $B$ is the largest $b_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.2619077559\n"
     ]
    }
   ],
   "source": [
    "def complete_ll(dt_matrix):\n",
    "    \"\"\"\n",
    "    Computes the total likelihood using\n",
    "    Pr(w_d) = sigma_k rho_k * Pi_v (beta_k,v)\n",
    "\n",
    "    and ll = \n",
    "    \"\"\"\n",
    "    ### Use log-sum trick\n",
    "    dt_tempZ = dt_matrix - np.max(dt_matrix,1)[:,np.newaxis]\n",
    "    ll = np.log(np.exp(dt_tempZ).sum(1)).sum()+dt_temp.max(1).sum()\n",
    "    return ll\n",
    "\n",
    "print(complete_ll(dt_temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-Step###\n",
    "\n",
    "Now we need to update our initial guesses of $\\rho$ and $\\beta$. \n",
    "\n",
    "1. First, let's update $\\rho$. \n",
    "\n",
    "We will use the formula:\n",
    "\n",
    "$\\rho_k^{new}=\\frac{\\Sigma_d\\hat{z_{d,k}}}{\\Sigma_k\\Sigma_d\\hat{z_{d,k}}}$ where $\\hat{z_{d,k}}=\\frac{\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}}}{\\Sigma_d \\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95618894  0.04381106]\n",
      " [ 0.64536243  0.35463757]\n",
      " [ 0.439352    0.560648  ]]\n"
     ]
    }
   ],
   "source": [
    "probs = np.exp(dt_temp)/np.exp(dt_temp).sum(1)[:,np.newaxis]\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row shows the distribution of a document across the K topics, where the rows sum to 1. For example, the top left element is how much [\"rugby\",\"football\",\"competition\",\"ball\",\"games\"] likes topic 1. \n",
    "\n",
    "We can just average each column across rows to come up with the new values of $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68030113  0.31969887]\n"
     ]
    }
   ],
   "source": [
    "rhos = probs.sum(0)/D\n",
    "\n",
    "print(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Second, let's update $\\beta$\n",
    "\n",
    "We will use the formula \n",
    "$\\beta_{k,v}^{new}=\\frac{\\Sigma_d\\hat{z_{d,k}}*x_{d,v}}{\\Sigma_d\\hat{z_{d,k}}*\\Sigma_vx_{d,v}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10484755  0.04817561  0.04817561  0.17561251  0.11894056  0.10484755\n",
      "   0.17561251  0.07076496  0.10484755  0.04817561]\n",
      " [ 0.01129093  0.14448955  0.14448955  0.10268772  0.23588633  0.01129093\n",
      "   0.10268772  0.09139678  0.01129093  0.14448955]]\n"
     ]
    }
   ],
   "source": [
    "betas_temp = np.dot(probs.T,feature_counts.values)\n",
    "betas=betas_temp/betas_temp.sum(1)[:,np.newaxis]\n",
    "\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's one iteration of the algorithm. The E-step and the M-step would be repeated until the log-likehood increases very litle. We can implement the entire code in the form of a class as follows:\n",
    "\n",
    "<a id='gimmethecode'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Author: Paul Soto \t\t  ###\n",
    "### \t\tpaul.soto@upf.edu ###\n",
    "#\t\t\t\t\t\t\t\t#\n",
    "# This file is a class to #######\n",
    "# perform the EM algorithm ######\n",
    "# on a multinomial distribution##\n",
    "#################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EM_algo():\n",
    "\t\"\"\"\"\n",
    "\tAlgorithm for the multinomial mixture model\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, data, K=2,\n",
    "\t\t\t\t\tprior_rhos=None, \n",
    "\t\t\t\t\tprior_betas=None):\n",
    "\t\tself.D = data.shape[0]\n",
    "\t\tself.V = data.shape[1]\n",
    "\t\tself.K = K\n",
    "\t\tself.data = data\n",
    "\t\tif prior_rhos==None:\n",
    "\t\t\tself.rhos = np.full(self.K, 1/float(self.K))\n",
    "\t\telse:\n",
    "\t\t\tself.rhos = prior_rhos\n",
    "\t\tif prior_betas==None:\n",
    "\t\t\tself.betas = np.random.dirichlet(self.V*[1], self.K)\n",
    "\t\telse:\n",
    "\t\t\tself.betas =prior_betas \n",
    "\t\tself.ll_history = []\n",
    "\n",
    "\tdef E_step(self):\n",
    "\n",
    "\t\tdef complete_ll(dt_matrix):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tComputes the total likelihood using\n",
    "\t\t\tPr(w_d) = sigma_k rho_k * Pi_v (beta_k,v)\n",
    "\n",
    "\t\t\tand ll = \n",
    "\t\t\t\"\"\"\n",
    "\t\t\t### Use log-sum trick\n",
    "\t\t\tdt_tempZ = dt_matrix - np.max(dt_matrix,1)[:,np.newaxis]\n",
    "\t\t\tll = np.log(np.exp(dt_tempZ).sum(1)).sum()+dt_temp.max(1).sum()\n",
    "\t\t\treturn ll\n",
    "\n",
    "\t\t### Get likelihood for each document-topic\n",
    "\t\tdt_temp = np.zeros((self.D,self.K))\n",
    "\t\tfor doc in range(self.D):\n",
    "\t\t\tfor topic in range(self.K):\n",
    "\t\t\t\tdt_temp[doc,topic] = np.log(self.rhos[topic]) + \\\n",
    "\t\t\t\t\t\t\t\t\t(self.data[doc]*np.log(self.betas[topic])).sum()\n",
    "\t\t### Compute complete log-likelihood\n",
    "\t\tself.ll = complete_ll(dt_temp)\n",
    "\t\tself.ll_history.append(self.ll)\n",
    "\n",
    "\t\t### normalize the Zs\n",
    "\t\tself.zs = np.exp(dt_temp)/np.exp(dt_temp).sum(1)[:,np.newaxis]\n",
    "\n",
    "\n",
    "\tdef M_step(self):\n",
    "\t\t### Update rhos\n",
    "\t\tself.rhos = self.zs.sum(0)/self.D\n",
    "\t\t### Update betas\n",
    "\t\tbetas_temp = np.dot(self.zs.T,self.data)\n",
    "\t\tself.betas=betas_temp/betas_temp.sum(1)[:,np.newaxis]\n",
    "\n",
    "\tdef iterate(self,tolerance=0.001,max_iter = 500):\n",
    "\t\tcount = 0\n",
    "\t\twhile True:\n",
    "\t\t\tself.E_step()\n",
    "\t\t\tself.M_step()\n",
    "\t\t\tif len(self.ll_history)==1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif (self.ll-self.ll_history[-2])<tolerance:\n",
    "\t\t\t\tprint(\"Arrived at target tolerance!\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif (count>max_iter):\n",
    "\t\t\t\tprint(\"Exceeded max_iterations!\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tcount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrived at target tolerance!\n",
      "[ 0.66666667  0.33333333]\n",
      "[[  0.00000000e+00   1.25000000e-01   1.25000000e-01   1.25000000e-01\n",
      "    2.50000000e-01   0.00000000e+00   1.25000000e-01   1.25000000e-01\n",
      "    0.00000000e+00   1.25000000e-01]\n",
      " [  2.00000000e-01   0.00000000e+00   0.00000000e+00   2.00000000e-01\n",
      "    5.07086456e-26   2.00000000e-01   2.00000000e-01   5.07086456e-26\n",
      "    2.00000000e-01   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "em_instance = EM_algo(feature_counts.values,K=2)\n",
    "\n",
    "em_instance.iterate()\n",
    "\n",
    "print(em_instance.rhos)\n",
    "print(em_instance.betas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
